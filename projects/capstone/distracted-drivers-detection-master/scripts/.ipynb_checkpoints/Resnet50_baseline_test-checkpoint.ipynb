{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage.io import imread, imshow\n",
    "import tensorflow as tf\n",
    "from preprocessing.preprocessing_factory import get_preprocessing\n",
    "from nets import nets_factory\n",
    "%matplotlib inline  \n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "from datasets import dataset_utils\n",
    "from datasets.dataset_utils import image_to_tfexample, _dataset_exists, _get_filenames_and_classes, write_label_file, ImageReader, _get_dataset_filename\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL = 'resnet_v2_50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir, tfrecord_filename, _NUM_SHARDS):\n",
    "  \"\"\"Converts the given filenames to a TFRecord dataset.\n",
    "  Args:\n",
    "    split_name: The name of the dataset, either 'train' or 'validation'.\n",
    "    filenames: A list of absolute paths to png or jpg images.\n",
    "    class_names_to_ids: A dictionary from class names (strings) to ids\n",
    "      (integers).\n",
    "    dataset_dir: The directory where the converted datasets are stored.\n",
    "  \"\"\"\n",
    "  assert split_name in ['train', 'validation', 'test']\n",
    "\n",
    "  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    image_reader = ImageReader()\n",
    "\n",
    "    with tf.Session('') as sess:\n",
    "\n",
    "      for shard_id in range(_NUM_SHARDS):\n",
    "        output_filename = _get_dataset_filename(\n",
    "            dataset_dir, split_name, shard_id, tfrecord_filename = tfrecord_filename, _NUM_SHARDS = _NUM_SHARDS)\n",
    "\n",
    "        with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
    "          start_ndx = shard_id * num_per_shard\n",
    "          end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n",
    "          for i in range(start_ndx, end_ndx):\n",
    "            sys.stdout.write('\\r>> Converting image {}/{} shard {}'.format(i+1, len(filenames), shard_id))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Read the filename:\n",
    "            image_data = tf.gfile.FastGFile(filenames[i], 'rb').read()\n",
    "            height, width = image_reader.read_image_dims(sess, image_data)\n",
    "            \n",
    "            if split_name != 'test':\n",
    "                class_name = os.path.basename(os.path.dirname(filenames[i]))\n",
    "                class_id = class_names_to_ids[class_name]\n",
    "            else:\n",
    "                img = os.path.splitext(os.path.basename(filenames[i]))[0]\n",
    "                class_id = int(img[4:])\n",
    "                \n",
    "            example = image_to_tfexample(image_data, 'jpg'.encode(), height, width, class_id)\n",
    "            tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "  sys.stdout.write('\\n')\n",
    "  sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = '/testset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 79726/79726 shard 39\n"
     ]
    }
   ],
   "source": [
    "_convert_dataset(split_name='test', \n",
    "                 filenames=filenames, \n",
    "                 class_names_to_ids=None, \n",
    "                 dataset_dir=dataset_dir, \n",
    "                 tfrecord_filename='drivers', \n",
    "                 _NUM_SHARDS=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_split(split_name, dataset_dir, file_pattern=None, file_pattern_for_counting='drivers', items_to_descriptions=None):\n",
    "    '''\n",
    "    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n",
    "    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
    "    Your file_pattern is very important in locating the files later.\n",
    "    INPUTS:\n",
    "    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "    - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "    - file_pattern_for_counting(str): the string name to identify your tfrecord files for counting\n",
    "    OUTPUTS:\n",
    "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
    "    '''\n",
    "\n",
    "    # First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation', 'test']:\n",
    "        raise ValueError('split name {} was not recognized.'.format(split_name))\n",
    "\n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = os.path.join(dataset_dir, file_pattern.format(split_name))\n",
    "\n",
    "    #Count the total number of examples in all of these shard\n",
    "    num_samples = 0\n",
    "    file_pattern_for_counting = file_pattern_for_counting + '_' + split_name\n",
    "    tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.startswith(file_pattern_for_counting)]\n",
    "    if len(tfrecords_to_count) == 0:\n",
    "        raise ValueError('There is no dataset.')\n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
    "            num_samples += 1\n",
    "\n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'image/format': tf.FixedLenFeature((), tf.string),\n",
    "      'image/class/label': tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64))\n",
    "    }\n",
    "\n",
    "    #Create the items_to_handlers dictionary for the decoder.\n",
    "    items_to_handlers = {\n",
    "    'image': slim.tfexample_decoder.Image(),\n",
    "    ''\n",
    "    'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "    \n",
    "    labels_to_names = None\n",
    "    if split_name != 'test':\n",
    "        #Create the labels_to_name file\n",
    "        if dataset_utils.has_labels(dataset_dir):\n",
    "            labels_to_names = dataset_utils.read_label_file(dataset_dir)\n",
    "\n",
    "        num_classes = len(labels_to_names)\n",
    "    else:\n",
    "        num_classes = None\n",
    "\n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = file_pattern_path,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        # num_readers = 4,\n",
    "        num_samples = num_samples,\n",
    "        num_classes = num_classes,\n",
    "        labels_to_names = labels_to_names,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batch(dataset, batch_size, MODEL, height, width, is_training=True):\n",
    "    '''\n",
    "    Loads a batch for training.\n",
    "    INPUTS:\n",
    "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
    "    - batch_size(int): determines how big of a batch to train\n",
    "    - height(int): the height of the image to resize to during preprocessing\n",
    "    - width(int): the width of the image to resize to during preprocessing\n",
    "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
    "    OUTPUTS:\n",
    "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
    "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
    "    '''\n",
    "    #First create the data_provider object\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset,\n",
    "        # common_queue_capacity = 24 + 3 * batch_size,\n",
    "        common_queue_capacity = 2 * batch_size,\n",
    "        common_queue_min = 1,\n",
    "        shuffle=is_training)\n",
    "\n",
    "    #Obtain the raw image using the get method\n",
    "    raw_image, label = data_provider.get(['image', 'label'])\n",
    "\n",
    "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "    preprocessing_fn = get_preprocessing(MODEL, is_training)\n",
    "    image = preprocessing_fn(raw_image, height, width)\n",
    "\n",
    "    #As for the raw images, we just do a simple reshape to batch it up\n",
    "    raw_image = tf.expand_dims(raw_image, 0)\n",
    "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
    "    raw_image = tf.squeeze(raw_image)\n",
    "\n",
    "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
    "    images, raw_images, labels = tf.train.batch(\n",
    "        [image, raw_image, label],\n",
    "        batch_size=batch_size,\n",
    "        # num_threads = 4,\n",
    "        num_threads=1,\n",
    "        # capacity = 4 * batch_size,\n",
    "        capacity=2 * batch_size,\n",
    "        allow_smaller_final_batch = True)\n",
    "\n",
    "    return images, raw_images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_pattern = 'drivers_{}_*.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nets_factory.get_network_fn(MODEL, num_classes=10, is_training=False)\n",
    "image_size = model.default_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#State where your log file is at. If it doesn't exist, create it.\n",
    "log_dir = '/log/' + MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the latest checkpoint file\n",
    "checkpoint_file = tf.train.latest_checkpoint(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_eval = 'log_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /log/resnet_v2_50/model.ckpt-14700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79726/79726 [30:42<00:00, 43.27it/s]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = get_split(split_name='test', \n",
    "                        dataset_dir=dataset_dir, \n",
    "                        file_pattern=file_pattern, \n",
    "                        file_pattern_for_counting='drivers', \n",
    "                        items_to_descriptions=None)\n",
    "    images, image_raw, labels = load_batch(dataset=dataset, \n",
    "                                           batch_size=1, \n",
    "                                           MODEL=MODEL, \n",
    "                                           height=image_size, \n",
    "                                           width=image_size, \n",
    "                                           is_training=False)\n",
    "    \n",
    "    # creat the modellog_eval = 'log_val'\n",
    "    logits, end_points = model(images)\n",
    "    \n",
    "    # define scopes \n",
    "    variables_to_restore = slim.get_variables_to_restore()\n",
    "    # create saver function to restore variables from a checkpoint file\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    def restore_fn(sess):\n",
    "        return saver.restore(sess, checkpoint_file)\n",
    "    \n",
    "#     # create global step for mornitoring\n",
    "#     global_step = get_or_create_global_step()\n",
    "#     global_step_op = tf.assign(global_step, global_step+1)\n",
    "    \n",
    "    \n",
    "    # the predictions\n",
    "    probabilities = end_points['predictions']\n",
    "    \n",
    "    sv = tf.train.Supervisor(logdir=log_eval, summary_op=None, saver=None, init_fn=restore_fn)\n",
    "    \n",
    "    with sv.managed_session() as sess:\n",
    "        predictions = [['img', 'c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']]\n",
    "        for i in tqdm(range(dataset.num_samples)):\n",
    "            p, label = sess.run([probabilities, labels])\n",
    "            p = list(p[0])\n",
    "            label = ['img_' + str(label[0]) + '.jpg']\n",
    "            p = label + p\n",
    "            predictions.append(p)\n",
    "        pd.DataFrame(predictions).to_csv('submission.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_90892.jpg</td>\n",
       "      <td>1.802470e-05</td>\n",
       "      <td>3.926010e-05</td>\n",
       "      <td>4.444770e-01</td>\n",
       "      <td>9.127220e-06</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>9.475340e-03</td>\n",
       "      <td>1.116580e-02</td>\n",
       "      <td>3.551730e-01</td>\n",
       "      <td>0.146571</td>\n",
       "      <td>0.032591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_90893.jpg</td>\n",
       "      <td>7.474290e-07</td>\n",
       "      <td>1.908600e-04</td>\n",
       "      <td>4.464620e-05</td>\n",
       "      <td>9.568100e-01</td>\n",
       "      <td>0.040360</td>\n",
       "      <td>4.195450e-07</td>\n",
       "      <td>1.032940e-05</td>\n",
       "      <td>7.093470e-05</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_90895.jpg</td>\n",
       "      <td>7.835570e-04</td>\n",
       "      <td>1.120520e-04</td>\n",
       "      <td>8.922430e-01</td>\n",
       "      <td>1.058480e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>4.476550e-06</td>\n",
       "      <td>1.463100e-04</td>\n",
       "      <td>9.266360e-04</td>\n",
       "      <td>0.105593</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_90896.jpg</td>\n",
       "      <td>1.646080e-05</td>\n",
       "      <td>9.980970e-01</td>\n",
       "      <td>7.735730e-06</td>\n",
       "      <td>4.388340e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>5.476100e-07</td>\n",
       "      <td>5.327090e-05</td>\n",
       "      <td>5.093550e-07</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_90897.jpg</td>\n",
       "      <td>4.001660e-02</td>\n",
       "      <td>5.166600e-10</td>\n",
       "      <td>5.264030e-08</td>\n",
       "      <td>2.206080e-04</td>\n",
       "      <td>0.046487</td>\n",
       "      <td>6.294720e-07</td>\n",
       "      <td>1.306170e-08</td>\n",
       "      <td>2.108920e-03</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.910233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             img            c0            c1            c2            c3  \\\n",
       "0  img_90892.jpg  1.802470e-05  3.926010e-05  4.444770e-01  9.127220e-06   \n",
       "1  img_90893.jpg  7.474290e-07  1.908600e-04  4.464620e-05  9.568100e-01   \n",
       "2  img_90895.jpg  7.835570e-04  1.120520e-04  8.922430e-01  1.058480e-05   \n",
       "3  img_90896.jpg  1.646080e-05  9.980970e-01  7.735730e-06  4.388340e-07   \n",
       "4  img_90897.jpg  4.001660e-02  5.166600e-10  5.264030e-08  2.206080e-04   \n",
       "\n",
       "         c4            c5            c6            c7        c8        c9  \n",
       "0  0.000481  9.475340e-03  1.116580e-02  3.551730e-01  0.146571  0.032591  \n",
       "1  0.040360  4.195450e-07  1.032940e-05  7.093470e-05  0.002456  0.000056  \n",
       "2  0.000004  4.476550e-06  1.463100e-04  9.266360e-04  0.105593  0.000176  \n",
       "3  0.000002  5.476100e-07  5.327090e-05  5.093550e-07  0.001791  0.000031  \n",
       "4  0.046487  6.294720e-07  1.306170e-08  2.108920e-03  0.000934  0.910233  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69756</th>\n",
       "      <td>img_1.jpg</td>\n",
       "      <td>1.497460e-01</td>\n",
       "      <td>9.004680e-06</td>\n",
       "      <td>2.210470e-05</td>\n",
       "      <td>3.267790e-05</td>\n",
       "      <td>3.598390e-05</td>\n",
       "      <td>2.834180e-01</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>5.957070e-03</td>\n",
       "      <td>0.268042</td>\n",
       "      <td>0.291779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69757</th>\n",
       "      <td>img_10.jpg</td>\n",
       "      <td>1.727530e-04</td>\n",
       "      <td>7.866160e-06</td>\n",
       "      <td>1.249400e-05</td>\n",
       "      <td>1.528170e-05</td>\n",
       "      <td>5.472760e-06</td>\n",
       "      <td>8.521920e-01</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>2.385580e-04</td>\n",
       "      <td>0.144901</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69758</th>\n",
       "      <td>img_100.jpg</td>\n",
       "      <td>9.711780e-01</td>\n",
       "      <td>9.697410e-03</td>\n",
       "      <td>1.700450e-04</td>\n",
       "      <td>2.508670e-03</td>\n",
       "      <td>1.154090e-02</td>\n",
       "      <td>1.296860e-05</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>1.359090e-03</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.002532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69759</th>\n",
       "      <td>img_1000.jpg</td>\n",
       "      <td>1.462590e-08</td>\n",
       "      <td>9.708770e-10</td>\n",
       "      <td>5.859830e-09</td>\n",
       "      <td>6.297600e-11</td>\n",
       "      <td>1.315720e-09</td>\n",
       "      <td>1.369930e-07</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>2.694590e-05</td>\n",
       "      <td>0.999590</td>\n",
       "      <td>0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69760</th>\n",
       "      <td>img_100000.jpg</td>\n",
       "      <td>5.994230e-05</td>\n",
       "      <td>2.677400e-06</td>\n",
       "      <td>3.409450e-06</td>\n",
       "      <td>9.971320e-01</td>\n",
       "      <td>1.511960e-04</td>\n",
       "      <td>8.694790e-06</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>5.183420e-07</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  img            c0            c1            c2            c3  \\\n",
       "69756       img_1.jpg  1.497460e-01  9.004680e-06  2.210470e-05  3.267790e-05   \n",
       "69757      img_10.jpg  1.727530e-04  7.866160e-06  1.249400e-05  1.528170e-05   \n",
       "69758     img_100.jpg  9.711780e-01  9.697410e-03  1.700450e-04  2.508670e-03   \n",
       "69759    img_1000.jpg  1.462590e-08  9.708770e-10  5.859830e-09  6.297600e-11   \n",
       "69760  img_100000.jpg  5.994230e-05  2.677400e-06  3.409450e-06  9.971320e-01   \n",
       "\n",
       "                 c4            c5        c6            c7        c8        c9  \n",
       "69756  3.598390e-05  2.834180e-01  0.000958  5.957070e-03  0.268042  0.291779  \n",
       "69757  5.472760e-06  8.521920e-01  0.002430  2.385580e-04  0.144901  0.000025  \n",
       "69758  1.154090e-02  1.296860e-05  0.000024  1.359090e-03  0.000977  0.002532  \n",
       "69759  1.315720e-09  1.369930e-07  0.000022  2.694590e-05  0.999590  0.000362  \n",
       "69760  1.511960e-04  8.694790e-06  0.002134  5.183420e-07  0.000500  0.000008  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values(by='img', ascending=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
