{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfer-learning\n",
    "model trained by the transfer-value from the pretrained models: Inception V3, Inception V4, Inception-Resnet-V2, Resnet 152."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "from datasets import dataset_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "from transfer_value_maker_loader import get_split_tv, load_batch_tv, read_shape_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL = 'inception_resnet_v2'\n",
    "# MODELs = ['inception_v3', \n",
    "#           'inception_v4', \n",
    "#           'inception_resnet_v2',\n",
    "#           'resnet_v2_152']\n",
    "\n",
    "dataset_dir = '../transfer-value-tf/' + MODEL\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "split_name = 'train'\n",
    "# split_names = ['train', 'validation', 'test']\n",
    "\n",
    "file_pattern_train = 'train*.tfrecord'\n",
    "file_pattern_val = 'validation*.tfrecord'\n",
    "file_pattern = split_name + '*.tfrecord'\n",
    "\n",
    "checkpoints_dir = '../checkpoints/' + MODEL\n",
    "\n",
    "log_dir = '../log/' + MODEL\n",
    "if not tf.gfile.Exists(log_dir):\n",
    "    tf.gfile.MakeDirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dropout_rate = 0.4\n",
    "\n",
    "fc_units = 512\n",
    "\n",
    "num_epochs_before_decay = 2\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "learning_rate_decay_factor = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2048]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = read_shape_file(MODEL, dataset_dir)\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(inputs, fc_units, num_classes, dropout_rate, is_training=True):\n",
    "    net = tf.squeeze(inputs, [1, 2], name='squeeze')\n",
    "    net = tf.layers.dense(net, fc_units, activation=tf.nn.relu, name='fc')\n",
    "    net = tf.layers.dropout(net, rate=dropout_rate, seed=42, training=is_training, name='dropout')\n",
    "    logits = tf.layers.dense(net, units=num_classes, name='logits')\n",
    "    probabilities = tf.nn.softmax(logits, name='softmax_output')\n",
    "    return logits, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_pre(inputs, fc_units, num_classes, dropout_rate, is_training=True):\n",
    "    net = tf.layers.dense(inputs, fc_units, activation=tf.nn.relu, name='fc')\n",
    "    net = tf.layers.dropout(net, rate=dropout_rate, seed=42, training=is_training, name='dropout')\n",
    "    logits = tf.layers.dense(net, units=num_classes, name='logits')\n",
    "    probabilities = tf.nn.softmax(logits, name='softmax_output')\n",
    "    return logits, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17940"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 0\n",
    "file_pattern_for_counting = 'train'\n",
    "tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.startswith(file_pattern_for_counting)]\n",
    "if len(tfrecords_to_count) == 0:\n",
    "    raise ValueError('There is no dataset.')\n",
    "for tfrecord_file in tfrecords_to_count:\n",
    "    for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
    "        num_samples += 1\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_tv(MODEL, split_name, dataset_dir): \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.logging.set_verbosity(tf.logging.INFO)\n",
    "        \n",
    "        file_pattern = split_name + '*.tfrecord'\n",
    "        dataset = get_split_tv(split_name, dataset_dir, file_pattern)\n",
    "        \n",
    "        shape = read_shape_file(MODEL, dataset_dir)\n",
    "        \n",
    "        arr_, label_, filename_ = load_batch_tv(dataset, 1, MODEL, shape, is_training=True)\n",
    "\n",
    "        sv = tf.train.Supervisor(logdir='../log/tmp')\n",
    "\n",
    "        with sv.managed_session() as sess:\n",
    "            list_ = []\n",
    "            for i in range(dataset.num_samples):\n",
    "                arrs, labels, filenames = sess.run([arr_, label_, filename_])\n",
    "                arr, label, filename = list(np.squeeze(arrs[0])), [labels[0]], [filenames[0].decode()]\n",
    "                row = filename + label + arr\n",
    "                list_.append(row)\n",
    "            df = pd.DataFrame(list_) \n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batches(dataset, batch_size):\n",
    "    num_samples = len(dataset)\n",
    "    num_batches = num_samples // batch_size + 1\n",
    "    for step in range(num_batches):\n",
    "        start = step * batch_size\n",
    "        end = min((step + 1) * batch_size, num_samples)\n",
    "        labels = dataset.iloc[start:end, 1]\n",
    "        inputs = dataset.iloc[start:end, 2:]\n",
    "        yield inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_val(dataset):\n",
    "    labels = dataset.iloc[:, 1]\n",
    "    inputs = dataset.iloc[:, 2:]\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dir(directory):\n",
    "    import shutil\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_dir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(MODEL, fc_units, num_epochs, batch_size, dropout_rate_train=0.4, initial_learning_rate=0.001, new_training=True):\n",
    "    dataset_dir = '../transfer-value-tf/' + MODEL\n",
    "    num_classes = 10\n",
    "    print(dataset_dir)\n",
    "\n",
    "    dataset_train = load_tv(MODEL, 'train', dataset_dir)\n",
    "    dataset_val = load_tv(MODEL, 'validation', dataset_dir)\n",
    "    \n",
    "    log_dir = '../log/' + MODEL\n",
    "    if new_training:\n",
    "        clean_dir(log_dir)\n",
    "    if not tf.gfile.Exists(log_dir):\n",
    "        tf.gfile.MakeDirs(log_dir)\n",
    "        \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "        shape = read_shape_file(MODEL, dataset_dir)\n",
    "\n",
    "        inputs = tf.placeholder(tf.float32, [None, shape[2]], name='input')\n",
    "        dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')\n",
    "        labels = tf.placeholder(tf.int32, name='labels')\n",
    "\n",
    "        logits, probs = model_pre(inputs, fc_units, num_classes, dropout_rate, is_training=True)\n",
    "\n",
    "        labels_oh = tf.contrib.layers.one_hot_encoding(labels, num_classes, scope='labels_oh')\n",
    "        # labels_oh = slim.one_hot_encoding(labels, num_classes, scope='labels_oh')\n",
    "\n",
    "\n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels=labels_oh, logits=logits)\n",
    "#         total_loss = tf.losses.get_total_loss()\n",
    "\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "#         global_step_op = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate=initial_learning_rate, \n",
    "                                                   global_step=global_step, \n",
    "                                                   decay_steps=17940*2, \n",
    "                                                   decay_rate=0.8, \n",
    "                                                   staircase=True)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "#         train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "        predictions = tf.argmax(probs, 1)\n",
    "        \n",
    "        # accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "        # metrics_op = tf.group(accuracy_update, probabilities)\n",
    "\n",
    "        _, accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions)\n",
    "\n",
    "        tf.summary.scalar('losses/Total_Loss', loss)\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "        my_summary_op = tf.summary.merge_all()\n",
    "        \n",
    "#         checkpoint_file = tf.train.latest_checkpoint(log_dir)\n",
    "#         saver = tf.train.Saver()       \n",
    "#         def restore_fn(sess):\n",
    "#             return saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        sv = tf.train.Supervisor(logdir=log_dir, summary_op=None)\n",
    "\n",
    "        with sv.managed_session() as sess:\n",
    "#         with tf.Session() as sess:\n",
    "            # Initializing the variables\n",
    "#             sess.run(tf.global_variables_initializer())\n",
    "            # saver.tf.train.Saver()\n",
    "#             step = 0\n",
    "            for i in range(num_epochs):\n",
    "                inputs_v, labels_v = load_val(dataset_val)\n",
    "#                 total_losses = []\n",
    "#                 total_accuracies = []\n",
    "                for inputs_t, labels_t in load_batches(dataset_train, batch_size=32):\n",
    "#                     step = sess.run(global_step_op)\n",
    "                    _, loss_t, acc_t = sess.run([optimizer, loss, accuracy], feed_dict={inputs: inputs_t, labels: labels_t, dropout_rate: dropout_rate_train})\n",
    "#                     summaries = sess.run(my_summary_op)\n",
    "#                     sv.summary_computed(sess, summaries)\n",
    "#                     if step % 10 == 0:\n",
    "#                         summaries = sess.run(my_summary_op)\n",
    "#                         sv.summary_computed(sess, summaries)\n",
    "# #                     total_losses.append(loss_t)\n",
    "#                     total_accuracies.append(acc_t)  \n",
    "                    # saver.save(sess, log_dir + '/model', global_step=step)\n",
    "#                 total_loss = np.mean(total_losses)\n",
    "#                 total_accuracy = np.mean(total_accuracies)\n",
    "                logging.info('Epoch {}/{}'.format(i, num_epochs))\n",
    "                loss_v, acc_v = sess.run([loss, accuracy], feed_dict={inputs: inputs_v, labels: labels_v, dropout_rate: 0})\n",
    "                logging.info('Training Loss: {}， Training Accuracy: {}.'.format(loss_t, acc_t))\n",
    "                logging.info('Validation Loss: {}, Validation Accuracy: {}.'.format(loss_v, acc_v))\n",
    "            logging.info('Training finished! Saving model to disk.')\n",
    "            sv.saver.save(sess, sv.save_path, global_step=sv.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../transfer-value-tf/inception_resnet_v2\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Recording summary at step None.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Recording summary at step None.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Saving checkpoint to path ../log/inception_resnet_v2\\model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Epoch 0/20\n",
      "INFO:tensorflow:Training Loss: 0.9082881212234497， Training Accuracy: 0.4591697156429291.\n",
      "INFO:tensorflow:Validation Loss: 0.6734882593154907, Validation Accuracy: 0.6357027888298035.\n",
      "INFO:tensorflow:Epoch 1/20\n",
      "INFO:tensorflow:Training Loss: 0.6417461037635803， Training Accuracy: 0.6772149205207825.\n",
      "INFO:tensorflow:Validation Loss: 0.44153261184692383, Validation Accuracy: 0.7268774509429932.\n",
      "INFO:tensorflow:Epoch 2/20\n",
      "INFO:tensorflow:Training Loss: 0.48804694414138794， Training Accuracy: 0.7475250959396362.\n",
      "INFO:tensorflow:Validation Loss: 0.39761683344841003, Validation Accuracy: 0.7719259262084961.\n",
      "INFO:tensorflow:Epoch 3/20\n",
      "INFO:tensorflow:Training Loss: 0.1876065731048584， Training Accuracy: 0.7843115925788879.\n",
      "INFO:tensorflow:Validation Loss: 0.2780826687812805, Validation Accuracy: 0.8016411066055298.\n",
      "INFO:tensorflow:Epoch 4/20\n",
      "INFO:tensorflow:Training Loss: 0.21317103505134583， Training Accuracy: 0.8100067973136902.\n",
      "INFO:tensorflow:Validation Loss: 0.25502991676330566, Validation Accuracy: 0.8216821551322937.\n",
      "INFO:tensorflow:Epoch 5/20\n",
      "INFO:tensorflow:Training Loss: 0.22254879772663116， Training Accuracy: 0.8278052806854248.\n",
      "INFO:tensorflow:Validation Loss: 0.20814530551433563, Validation Accuracy: 0.8369678258895874.\n",
      "INFO:tensorflow:Epoch 6/20\n",
      "INFO:tensorflow:Training Loss: 0.17015564441680908， Training Accuracy: 0.8419594168663025.\n",
      "INFO:tensorflow:Validation Loss: 0.23459339141845703, Validation Accuracy: 0.8486379384994507.\n",
      "INFO:tensorflow:Epoch 7/20\n",
      "INFO:tensorflow:Training Loss: 0.21075782179832458， Training Accuracy: 0.852257251739502.\n",
      "INFO:tensorflow:Validation Loss: 0.2427029013633728, Validation Accuracy: 0.8575354814529419.\n",
      "INFO:tensorflow:Epoch 8/20\n",
      "INFO:tensorflow:Training Loss: 0.31609585881233215， Training Accuracy: 0.8607560992240906.\n",
      "INFO:tensorflow:Validation Loss: 0.16443029046058655, Validation Accuracy: 0.8656548261642456.\n",
      "INFO:tensorflow:Epoch 9/20\n",
      "INFO:tensorflow:Training Loss: 0.1232972964644432， Training Accuracy: 0.8682904839515686.\n",
      "INFO:tensorflow:Validation Loss: 0.1670958697795868, Validation Accuracy: 0.8722395896911621.\n",
      "INFO:tensorflow:Epoch 10/20\n",
      "INFO:tensorflow:Training Loss: 0.10903500020503998， Training Accuracy: 0.8745071887969971.\n",
      "INFO:tensorflow:Validation Loss: 0.17584526538848877, Validation Accuracy: 0.8779473304748535.\n",
      "INFO:tensorflow:Epoch 11/20\n",
      "INFO:tensorflow:Training Loss: 0.12572912871837616， Training Accuracy: 0.8798556923866272.\n",
      "INFO:tensorflow:Validation Loss: 0.1562938243150711, Validation Accuracy: 0.8828078508377075.\n",
      "INFO:tensorflow:Epoch 12/20\n",
      "INFO:tensorflow:Training Loss: 0.27200570702552795， Training Accuracy: 0.8845219016075134.\n",
      "INFO:tensorflow:Validation Loss: 0.1557733565568924, Validation Accuracy: 0.887140154838562.\n",
      "INFO:tensorflow:Epoch 13/20\n",
      "INFO:tensorflow:Training Loss: 0.06006395071744919， Training Accuracy: 0.8886229395866394.\n",
      "INFO:tensorflow:Validation Loss: 0.15372347831726074, Validation Accuracy: 0.8909841775894165.\n",
      "INFO:tensorflow:Epoch 14/20\n",
      "INFO:tensorflow:Training Loss: 0.07607860863208771， Training Accuracy: 0.8923032879829407.\n",
      "INFO:tensorflow:Validation Loss: 0.16170524060726166, Validation Accuracy: 0.8943513035774231.\n",
      "INFO:tensorflow:Epoch 15/20\n",
      "INFO:tensorflow:Training Loss: 0.16148531436920166， Training Accuracy: 0.8955772519111633.\n",
      "INFO:tensorflow:Validation Loss: 0.14827416837215424, Validation Accuracy: 0.8974006772041321.\n",
      "INFO:tensorflow:Epoch 16/20\n",
      "INFO:tensorflow:Training Loss: 0.12476052343845367， Training Accuracy: 0.8983876705169678.\n",
      "INFO:tensorflow:Validation Loss: 0.14051969349384308, Validation Accuracy: 0.9000231027603149.\n",
      "INFO:tensorflow:Epoch 17/20\n",
      "INFO:tensorflow:Training Loss: 0.10247862339019775， Training Accuracy: 0.9010346531867981.\n",
      "INFO:tensorflow:Validation Loss: 0.11110148578882217, Validation Accuracy: 0.9026514291763306.\n",
      "INFO:tensorflow:Epoch 18/20\n",
      "INFO:tensorflow:Training Loss: 0.12515375018119812， Training Accuracy: 0.9036526679992676.\n",
      "INFO:tensorflow:Validation Loss: 0.14789308607578278, Validation Accuracy: 0.9051298499107361.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Epoch 19/20\n",
      "INFO:tensorflow:Training Loss: 0.12264274060726166， Training Accuracy: 0.905907154083252.\n",
      "INFO:tensorflow:Validation Loss: 0.13384723663330078, Validation Accuracy: 0.9071642160415649.\n",
      "INFO:tensorflow:Training finished! Saving model to disk.\n"
     ]
    }
   ],
   "source": [
    "train(MODEL='inception_resnet_v2', \n",
    "      fc_units=512, \n",
    "      num_epochs=20, \n",
    "      batch_size=64, \n",
    "      dropout_rate_train=0.4, \n",
    "      initial_learning_rate=0.001, \n",
    "      new_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(MODEL, fc_units, num_epochs, batch_size, dropout_rate_train=0.4, initial_learning_rate=0.001, new_training=True):\n",
    "    dataset_dir = '../transfer-value-tf/' + MODEL\n",
    "    num_classes = 10\n",
    "    print(dataset_dir)\n",
    "\n",
    "    dataset_train = load_tv(MODEL, 'train', dataset_dir)\n",
    "    dataset_val = load_tv(MODEL, 'validation', dataset_dir)\n",
    "    \n",
    "    log_dir = '../log/' + MODEL\n",
    "    if new_training:\n",
    "        clean_dir(log_dir)\n",
    "    if not tf.gfile.Exists(log_dir):\n",
    "        tf.gfile.MakeDirs(log_dir)\n",
    "        \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "        shape = read_shape_file(MODEL, dataset_dir)\n",
    "\n",
    "        inputs = tf.placeholder(tf.float32, [None, shape[2]], name='input')\n",
    "        dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')\n",
    "        labels = tf.placeholder(tf.int32, name='labels')\n",
    "\n",
    "        logits, probs = model_pre(inputs, fc_units, num_classes, dropout_rate, is_training=True)\n",
    "\n",
    "        labels_oh = tf.contrib.layers.one_hot_encoding(labels, num_classes, scope='labels_oh')\n",
    "        # labels_oh = slim.one_hot_encoding(labels, num_classes, scope='labels_oh')\n",
    "\n",
    "\n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels=labels_oh, logits=logits)\n",
    "        total_loss = tf.losses.get_total_loss()\n",
    "\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "#         global_step_op = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate=initial_learning_rate, \n",
    "                                                   global_step=global_step, \n",
    "                                                   decay_steps=17940*2, \n",
    "                                                   decay_rate=0.8, \n",
    "                                                   staircase=True)\n",
    "\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "        predictions = tf.argmax(probs, 1)\n",
    "        \n",
    "        # accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "        # metrics_op = tf.group(accuracy_update, probabilities)\n",
    "\n",
    "        _, accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions)\n",
    "\n",
    "        tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "        my_summary_op = tf.summary.merge_all()\n",
    "        \n",
    "#         checkpoint_file = tf.train.latest_checkpoint(log_dir)\n",
    "#         saver = tf.train.Saver()       \n",
    "#         def restore_fn(sess):\n",
    "#             return saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        sv = tf.train.Supervisor(logdir=log_dir, summary_op=None)\n",
    "\n",
    "        with sv.managed_session() as sess:\n",
    "#         with tf.Session() as sess:\n",
    "            # Initializing the variables\n",
    "#             sess.run(tf.global_variables_initializer())\n",
    "            # saver.tf.train.Saver()\n",
    "#             step = 0\n",
    "            for i in range(num_epochs):\n",
    "                inputs_v, labels_v = load_val(dataset_val)\n",
    "#                 total_losses = []\n",
    "#                 total_accuracies = []\n",
    "                for inputs_t, labels_t in load_batches(dataset_train, batch_size=32):\n",
    "#                     step = sess.run(global_step_op)\n",
    "#                     _, loss_t, acc_t = sess.run([optimizer, loss, accuracy], feed_dict={inputs: inputs_t, labels: labels_t, dropout_rate: dropout_rate_train})\n",
    "                    loss_t, acc_t, step = sess.run([train_op, accuracy, sv.global_step], feed_dict={inputs: inputs_t, labels: labels_t, dropout_rate: dropout_rate_train})\n",
    "                    \n",
    "                    if step % 10 == 0:\n",
    "                        summaries = sess.run(my_summary_op, feed_dict={inputs: inputs_t, labels: labels_t, dropout_rate: dropout_rate_train})\n",
    "                        sv.summary_computed(sess, summaries)\n",
    "# #                     total_losses.append(loss_t)\n",
    "#                     total_accuracies.append(acc_t)  \n",
    "                    # saver.save(sess, log_dir + '/model', global_step=step)\n",
    "#                 total_loss = np.mean(total_losses)\n",
    "#                 total_accuracy = np.mean(total_accuracies)\n",
    "                logging.info('Epoch {}/{}'.format(i, num_epochs))\n",
    "                loss_v, acc_v = sess.run([loss, accuracy], feed_dict={inputs: inputs_v, labels: labels_v, dropout_rate: 0})\n",
    "                logging.info('Training Loss: {}， Training Accuracy: {}.'.format(loss_t, acc_t))\n",
    "                logging.info('Validation Loss: {}, Validation Accuracy: {}.'.format(loss_v, acc_v))\n",
    "            logging.info('Training finished! Saving model to disk.')\n",
    "            sv.saver.save(sess, sv.save_path, global_step=sv.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../transfer-value-tf/inception_v3\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Recording summary at step None.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Recording summary at step None.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path ../log/inception_v3\\model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Epoch 0/20\n",
      "INFO:tensorflow:Training Loss: 0.9491729736328125， Training Accuracy: 0.6577640175819397.\n",
      "INFO:tensorflow:Validation Loss: 0.4167082607746124, Validation Accuracy: 0.7027584910392761.\n",
      "INFO:tensorflow:Epoch 1/20\n",
      "INFO:tensorflow:Training Loss: 0.5003495216369629， Training Accuracy: 0.770706295967102.\n",
      "INFO:tensorflow:Validation Loss: 0.2526908814907074, Validation Accuracy: 0.7860918641090393.\n",
      "INFO:tensorflow:Epoch 2/20\n",
      "INFO:tensorflow:Training Loss: 0.3502574563026428， Training Accuracy: 0.8171322345733643.\n",
      "INFO:tensorflow:Validation Loss: 0.19532810151576996, Validation Accuracy: 0.8252120018005371.\n",
      "INFO:tensorflow:Epoch 3/20\n",
      "INFO:tensorflow:Training Loss: 0.3775855302810669， Training Accuracy: 0.8426499366760254.\n",
      "INFO:tensorflow:Validation Loss: 0.17704153060913086, Validation Accuracy: 0.8476007580757141.\n",
      "INFO:tensorflow:Epoch 4/20\n",
      "INFO:tensorflow:Training Loss: 0.2978036105632782， Training Accuracy: 0.8596349954605103.\n",
      "INFO:tensorflow:Validation Loss: 0.1526954025030136, Validation Accuracy: 0.8633217811584473.\n",
      "INFO:tensorflow:Epoch 5/20\n",
      "INFO:tensorflow:Training Loss: 0.30488598346710205， Training Accuracy: 0.8717510104179382.\n",
      "INFO:tensorflow:Validation Loss: 0.15852901339530945, Validation Accuracy: 0.8744081258773804.\n",
      "INFO:tensorflow:Epoch 6/20\n",
      "INFO:tensorflow:Training Loss: 0.4138687551021576， Training Accuracy: 0.8813413381576538.\n",
      "INFO:tensorflow:Validation Loss: 0.14327019453048706, Validation Accuracy: 0.8833829164505005.\n",
      "INFO:tensorflow:Epoch 7/20\n",
      "INFO:tensorflow:Training Loss: 0.21758529543876648， Training Accuracy: 0.8879066109657288.\n",
      "INFO:tensorflow:Validation Loss: 0.1141301691532135, Validation Accuracy: 0.8897475004196167.\n",
      "INFO:tensorflow:Epoch 8/20\n",
      "INFO:tensorflow:Training Loss: 0.3239300549030304， Training Accuracy: 0.893928587436676.\n",
      "INFO:tensorflow:Validation Loss: 0.13884983956813812, Validation Accuracy: 0.8953676223754883.\n",
      "INFO:tensorflow:Epoch 9/20\n",
      "INFO:tensorflow:Training Loss: 0.15559172630310059， Training Accuracy: 0.8987823724746704.\n",
      "INFO:tensorflow:Validation Loss: 0.10190018266439438, Validation Accuracy: 0.900177538394928.\n",
      "INFO:tensorflow:Epoch 10/20\n",
      "INFO:tensorflow:Training Loss: 0.19593545794487， Training Accuracy: 0.903398871421814.\n",
      "INFO:tensorflow:Validation Loss: 0.0997544378042221, Validation Accuracy: 0.9045330882072449.\n",
      "INFO:tensorflow:Epoch 11/20\n",
      "INFO:tensorflow:Training Loss: 0.03396879881620407， Training Accuracy: 0.9073002338409424.\n",
      "INFO:tensorflow:Validation Loss: 0.09196562319993973, Validation Accuracy: 0.908348560333252.\n",
      "INFO:tensorflow:Epoch 12/20\n",
      "INFO:tensorflow:Training Loss: 0.3416770398616791， Training Accuracy: 0.9107635617256165.\n",
      "INFO:tensorflow:Validation Loss: 0.11653353273868561, Validation Accuracy: 0.9115675687789917.\n",
      "INFO:tensorflow:Epoch 13/20\n",
      "INFO:tensorflow:Training Loss: 0.24578765034675598， Training Accuracy: 0.9138569831848145.\n",
      "INFO:tensorflow:Validation Loss: 0.08838003873825073, Validation Accuracy: 0.9146541357040405.\n",
      "INFO:tensorflow:Epoch 14/20\n",
      "INFO:tensorflow:Training Loss: 0.21200549602508545， Training Accuracy: 0.9164854884147644.\n",
      "INFO:tensorflow:Validation Loss: 0.12384229153394699, Validation Accuracy: 0.9170511364936829.\n",
      "INFO:tensorflow:Epoch 15/20\n",
      "INFO:tensorflow:Training Loss: 0.09704427421092987， Training Accuracy: 0.9187372326850891.\n",
      "INFO:tensorflow:Validation Loss: 0.08566439896821976, Validation Accuracy: 0.9194014668464661.\n",
      "INFO:tensorflow:Epoch 16/20\n",
      "INFO:tensorflow:Training Loss: 0.25771766901016235， Training Accuracy: 0.9209495186805725.\n",
      "INFO:tensorflow:Validation Loss: 0.10215310007333755, Validation Accuracy: 0.9214898347854614.\n",
      "INFO:tensorflow:Epoch 17/20\n",
      "INFO:tensorflow:Training Loss: 0.14576487243175507， Training Accuracy: 0.9227697253227234.\n",
      "INFO:tensorflow:Validation Loss: 0.08017756044864655, Validation Accuracy: 0.9233301281929016.\n",
      "INFO:tensorflow:Epoch 18/20\n",
      "INFO:tensorflow:Training Loss: 0.18315358459949493， Training Accuracy: 0.9245017766952515.\n",
      "INFO:tensorflow:Validation Loss: 0.08926087617874146, Validation Accuracy: 0.924972414970398.\n",
      "INFO:tensorflow:Epoch 19/20\n",
      "INFO:tensorflow:Training Loss: 0.26935067772865295， Training Accuracy: 0.9260444641113281.\n",
      "INFO:tensorflow:Validation Loss: 0.080305315554142, Validation Accuracy: 0.9265009760856628.\n",
      "INFO:tensorflow:Training finished! Saving model to disk.\n"
     ]
    }
   ],
   "source": [
    "train(MODEL='inception_v3', \n",
    "      fc_units=512, \n",
    "      num_epochs=20, \n",
    "      batch_size=64, \n",
    "      dropout_rate_train=0.4, \n",
    "      initial_learning_rate=0.001, \n",
    "      new_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
