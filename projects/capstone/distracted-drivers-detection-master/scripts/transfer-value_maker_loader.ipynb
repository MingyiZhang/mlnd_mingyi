{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer-value maker and Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to save a lot of time in performing transfer learning, one can store the transfer-value of images, which is the output of the pretrained model layer before the logit layer. In this notebook, I will go through this procedure and define all necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "\n",
    "from preprocessing.preprocessing_factory import get_preprocessing\n",
    "from nets import nets_factory\n",
    "\n",
    "from checkpoints_downloader import ckpt_maker\n",
    "from dataset_preparation import get_split, load_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL = 'resnet_v2_50'\n",
    "\n",
    "tf_dir = '../transfer-value-tf/' + MODEL\n",
    "if not tf.gfile.Exists(tf_dir):\n",
    "    tf.gfile.MakeDirs(tf_dir)\n",
    "    \n",
    "dataset_dir = '../dataset_tf'\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "file_pattern = 'drivers_{}_*.tfrecord'\n",
    "\n",
    "checkpoints_dir = '../checkpoints'\n",
    "\n",
    "log_dir = '../log/transfer-value/' + MODEL\n",
    "if not tf.gfile.Exists(log_dir):\n",
    "    tf.gfile.MakeDirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read out the structure of model\n",
    "def model_structure(MODEL, num_classes=10):\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        model = nets_factory.get_network_fn(MODEL, num_classes=num_classes, is_training=False)\n",
    "        # get image size\n",
    "        image_size = model.default_image_size\n",
    "\n",
    "        inputs = tf.placeholder(tf.float32, [None, image_size, image_size, 3], name='input')\n",
    "\n",
    "        _, endpoints = model(inputs)\n",
    "\n",
    "        for k in endpoints:\n",
    "            print('layer_scope: {}\\n layer_name: {}\\nlayer_shape: {}'.format(endpoints[k].name, k, endpoints[k].get_shape()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "layer_scope: resnet_v2_50/conv1/BiasAdd:0\n",
      " layer_name: resnet_v2_50/conv1\n",
      "layer_shape: (?, 112, 112, 64)\n",
      "layer_scope: resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut\n",
      "layer_shape: (?, 56, 56, 256)\n",
      "layer_scope: resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block1/unit_1/bottleneck_v2/conv1\n",
      "layer_shape: (?, 56, 56, 64)\n",
      "layer_scope: resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block1/unit_1/bottleneck_v2/conv2\n",
      "layer_shape: (?, 56, 56, 64)\n",
      "layer_scope: resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block1/unit_1/bottleneck_v2/conv3\n",
      "layer_shape: (?, 56, 56, 256)\n",
      "layer_scope: resnet_v2_50/block1/unit_1/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block1/unit_1/bottleneck_v2\n",
      "layer_shape: (?, 56, 56, 256)\n",
      "layer_scope: resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block1/unit_2/bottleneck_v2/conv1\n",
      "layer_shape: (?, 56, 56, 64)\n",
      "layer_scope: resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block1/unit_2/bottleneck_v2/conv2\n",
      "layer_shape: (?, 56, 56, 64)\n",
      "layer_scope: resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block1/unit_2/bottleneck_v2/conv3\n",
      "layer_shape: (?, 56, 56, 256)\n",
      "layer_scope: resnet_v2_50/block1/unit_2/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block1/unit_2/bottleneck_v2\n",
      "layer_shape: (?, 56, 56, 256)\n",
      "layer_scope: resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block1/unit_3/bottleneck_v2/conv1\n",
      "layer_shape: (?, 56, 56, 64)\n",
      "layer_scope: resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block1/unit_3/bottleneck_v2/conv2\n",
      "layer_shape: (?, 28, 28, 64)\n",
      "layer_scope: resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block1/unit_3/bottleneck_v2/conv3\n",
      "layer_shape: (?, 28, 28, 256)\n",
      "layer_scope: resnet_v2_50/block1/unit_3/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block1/unit_3/bottleneck_v2\n",
      "layer_shape: (?, 28, 28, 256)\n",
      "layer_scope: resnet_v2_50/block1/unit_3/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block1\n",
      "layer_shape: (?, 28, 28, 256)\n",
      "layer_scope: resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut\n",
      "layer_shape: (?, 28, 28, 512)\n",
      "layer_scope: resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block2/unit_1/bottleneck_v2/conv1\n",
      "layer_shape: (?, 28, 28, 128)\n",
      "layer_scope: resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block2/unit_1/bottleneck_v2/conv2\n",
      "layer_shape: (?, 28, 28, 128)\n",
      "layer_scope: resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block2/unit_1/bottleneck_v2/conv3\n",
      "layer_shape: (?, 28, 28, 512)\n",
      "layer_scope: resnet_v2_50/block2/unit_1/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block2/unit_1/bottleneck_v2\n",
      "layer_shape: (?, 28, 28, 512)\n",
      "layer_scope: resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block2/unit_2/bottleneck_v2/conv1\n",
      "layer_shape: (?, 28, 28, 128)\n",
      "layer_scope: resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block2/unit_2/bottleneck_v2/conv2\n",
      "layer_shape: (?, 28, 28, 128)\n",
      "layer_scope: resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block2/unit_2/bottleneck_v2/conv3\n",
      "layer_shape: (?, 28, 28, 512)\n",
      "layer_scope: resnet_v2_50/block2/unit_2/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block2/unit_2/bottleneck_v2\n",
      "layer_shape: (?, 28, 28, 512)\n",
      "layer_scope: resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block2/unit_3/bottleneck_v2/conv1\n",
      "layer_shape: (?, 28, 28, 128)\n",
      "layer_scope: resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block2/unit_3/bottleneck_v2/conv2\n",
      "layer_shape: (?, 28, 28, 128)\n",
      "layer_scope: resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block2/unit_3/bottleneck_v2/conv3\n",
      "layer_shape: (?, 28, 28, 512)\n",
      "layer_scope: resnet_v2_50/block2/unit_3/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block2/unit_3/bottleneck_v2\n",
      "layer_shape: (?, 28, 28, 512)\n",
      "layer_scope: resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block2/unit_4/bottleneck_v2/conv1\n",
      "layer_shape: (?, 28, 28, 128)\n",
      "layer_scope: resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block2/unit_4/bottleneck_v2/conv2\n",
      "layer_shape: (?, 14, 14, 128)\n",
      "layer_scope: resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block2/unit_4/bottleneck_v2/conv3\n",
      "layer_shape: (?, 14, 14, 512)\n",
      "layer_scope: resnet_v2_50/block2/unit_4/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block2/unit_4/bottleneck_v2\n",
      "layer_shape: (?, 14, 14, 512)\n",
      "layer_scope: resnet_v2_50/block2/unit_4/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block2\n",
      "layer_shape: (?, 14, 14, 512)\n",
      "layer_scope: resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut\n",
      "layer_shape: (?, 14, 14, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_1/bottleneck_v2/conv1\n",
      "layer_shape: (?, 14, 14, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_1/bottleneck_v2/conv2\n",
      "layer_shape: (?, 14, 14, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block3/unit_1/bottleneck_v2/conv3\n",
      "layer_shape: (?, 14, 14, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_1/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block3/unit_1/bottleneck_v2\n",
      "layer_shape: (?, 14, 14, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_2/bottleneck_v2/conv1\n",
      "layer_shape: (?, 14, 14, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_2/bottleneck_v2/conv2\n",
      "layer_shape: (?, 14, 14, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block3/unit_2/bottleneck_v2/conv3\n",
      "layer_shape: (?, 14, 14, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_2/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block3/unit_2/bottleneck_v2\n",
      "layer_shape: (?, 14, 14, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_3/bottleneck_v2/conv1\n",
      "layer_shape: (?, 14, 14, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_3/bottleneck_v2/conv2\n",
      "layer_shape: (?, 14, 14, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block3/unit_3/bottleneck_v2/conv3\n",
      "layer_shape: (?, 14, 14, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_3/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block3/unit_3/bottleneck_v2\n",
      "layer_shape: (?, 14, 14, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_4/bottleneck_v2/conv1\n",
      "layer_shape: (?, 14, 14, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_4/bottleneck_v2/conv2\n",
      "layer_shape: (?, 14, 14, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block3/unit_4/bottleneck_v2/conv3\n",
      "layer_shape: (?, 14, 14, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_4/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block3/unit_4/bottleneck_v2\n",
      "layer_shape: (?, 14, 14, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_5/bottleneck_v2/conv1\n",
      "layer_shape: (?, 14, 14, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_5/bottleneck_v2/conv2\n",
      "layer_shape: (?, 14, 14, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block3/unit_5/bottleneck_v2/conv3\n",
      "layer_shape: (?, 14, 14, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_5/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block3/unit_5/bottleneck_v2\n",
      "layer_shape: (?, 14, 14, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_6/bottleneck_v2/conv1\n",
      "layer_shape: (?, 14, 14, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block3/unit_6/bottleneck_v2/conv2\n",
      "layer_shape: (?, 7, 7, 256)\n",
      "layer_scope: resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block3/unit_6/bottleneck_v2/conv3\n",
      "layer_shape: (?, 7, 7, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_6/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block3/unit_6/bottleneck_v2\n",
      "layer_shape: (?, 7, 7, 1024)\n",
      "layer_scope: resnet_v2_50/block3/unit_6/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block3\n",
      "layer_shape: (?, 7, 7, 1024)\n",
      "layer_scope: resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut\n",
      "layer_shape: (?, 7, 7, 2048)\n",
      "layer_scope: resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block4/unit_1/bottleneck_v2/conv1\n",
      "layer_shape: (?, 7, 7, 512)\n",
      "layer_scope: resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block4/unit_1/bottleneck_v2/conv2\n",
      "layer_shape: (?, 7, 7, 512)\n",
      "layer_scope: resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block4/unit_1/bottleneck_v2/conv3\n",
      "layer_shape: (?, 7, 7, 2048)\n",
      "layer_scope: resnet_v2_50/block4/unit_1/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block4/unit_1/bottleneck_v2\n",
      "layer_shape: (?, 7, 7, 2048)\n",
      "layer_scope: resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block4/unit_2/bottleneck_v2/conv1\n",
      "layer_shape: (?, 7, 7, 512)\n",
      "layer_scope: resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block4/unit_2/bottleneck_v2/conv2\n",
      "layer_shape: (?, 7, 7, 512)\n",
      "layer_scope: resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block4/unit_2/bottleneck_v2/conv3\n",
      "layer_shape: (?, 7, 7, 2048)\n",
      "layer_scope: resnet_v2_50/block4/unit_2/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block4/unit_2/bottleneck_v2\n",
      "layer_shape: (?, 7, 7, 2048)\n",
      "layer_scope: resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/Relu:0\n",
      " layer_name: resnet_v2_50/block4/unit_3/bottleneck_v2/conv1\n",
      "layer_shape: (?, 7, 7, 512)\n",
      "layer_scope: resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/Relu:0\n",
      " layer_name: resnet_v2_50/block4/unit_3/bottleneck_v2/conv2\n",
      "layer_shape: (?, 7, 7, 512)\n",
      "layer_scope: resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/BiasAdd:0\n",
      " layer_name: resnet_v2_50/block4/unit_3/bottleneck_v2/conv3\n",
      "layer_shape: (?, 7, 7, 2048)\n",
      "layer_scope: resnet_v2_50/block4/unit_3/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block4/unit_3/bottleneck_v2\n",
      "layer_shape: (?, 7, 7, 2048)\n",
      "layer_scope: resnet_v2_50/block4/unit_3/bottleneck_v2/add:0\n",
      " layer_name: resnet_v2_50/block4\n",
      "layer_shape: (?, 7, 7, 2048)\n",
      "layer_scope: resnet_v2_50/logits/BiasAdd:0\n",
      " layer_name: resnet_v2_50/logits\n",
      "layer_shape: (?, 1, 1, 10)\n",
      "layer_scope: resnet_v2_50/predictions/Reshape_1:0\n",
      " layer_name: predictions\n",
      "layer_shape: (?, 10)\n"
     ]
    }
   ],
   "source": [
    "model_structure('resnet_v2_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give the scope of the layer before fully-connected layer or logits\n",
    "def base_layer(MODEL):\n",
    "    nets_base_map = {'vgg_16': 'vgg_16/pool5',\n",
    "                     'vgg_19': 'vgg_19/pool5',\n",
    "                     'inception_v1': 'Mixed_5c',\n",
    "                     'inception_v2': 'Mixed_5c',\n",
    "                     'inception_v3': 'Mixed_7c',\n",
    "                     'inception_v4': 'Mixed_7d',\n",
    "                     'inception_resnet_v2': 'Conv2d_7b_1x1',\n",
    "                     'resnet_v2_50': 'resnet_v2_50/block4',\n",
    "                     'resnet_v2_101': 'resnet_v2_101/block4',\n",
    "                     'resnet_v2_152': 'resnet_v2_152/block4',\n",
    "                     }\n",
    "    return nets_base_map[MODEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exclude_scope(MODEL):\n",
    "    nets_exclude_map = {'vgg_16': ['vgg_16/fc6', 'vgg_16/fc7', 'vgg_16/fc8'],\n",
    "                        'vgg_19': ['vgg_19/fc6', 'vgg_19/fc7', 'vgg_19/fc8'],\n",
    "                        'inception_v1': ['InceptionV1/AuxLogits', 'InceptionV1/Logits'],\n",
    "                        'inception_v2': ['InceptionV2/AuxLogits', 'InceptionV2/Logits'],\n",
    "                        'inception_v3': ['InceptionV3/AuxLogits', 'InceptionV3/Logits'],\n",
    "                        'inception_v4': ['InceptionV4/AuxLogits', 'InceptionV4/Logits'],\n",
    "                        'inception_resnet_v2': ['InceptionResnetV2/AuxLogits', 'InceptionResnetV2/Logits'],\n",
    "                        'resnet_v2_50': ['resnet_v2_50/logits'],\n",
    "                        'resnet_v2_101': ['resnet_v2_101/logits'],\n",
    "                        'resnet_v2_152': ['resnet_v2_152/logits'],\n",
    "                        }\n",
    "    return nets_exclude_map[MODEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "resnet_v2_50/conv1/weights:0\n",
      "resnet_v2_50/conv1/biases:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/biases:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/biases:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/preact/moving_mean:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/preact/moving_variance:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights:0\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases:0\n",
      "resnet_v2_50/postnorm/beta:0\n",
      "resnet_v2_50/postnorm/gamma:0\n",
      "resnet_v2_50/postnorm/moving_mean:0\n",
      "resnet_v2_50/postnorm/moving_variance:0\n",
      "resnet_v2_50/logits/weights:0\n",
      "resnet_v2_50/logits/biases:0\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    model = nets_factory.get_network_fn('resnet_v2_50', num_classes=10, is_training=False)\n",
    "    # get image size\n",
    "    image_size = model.default_image_size\n",
    "\n",
    "    inputs = tf.placeholder(tf.float32, [None, image_size, image_size, 3], name='input')\n",
    "\n",
    "    _, endpoints = model(inputs)\n",
    "\n",
    "    # exclude = exclude_layer(endpoints)\n",
    "    \n",
    "    variables = slim.get_variables_to_restore()\n",
    "    for var in variables:\n",
    "        print(var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_shape_file(shape, MODEL, dataset_dir):\n",
    "    '''Write a file with the shape of the output of MODEL base.\n",
    "    \n",
    "    Args: \n",
    "        shape(tuple): (height, width, channel)\n",
    "        MODEL(str): model name \n",
    "        dataset_dir(str): file directory\n",
    "    '''\n",
    "    filename = 'shape_' + MODEL + '.txt'\n",
    "    file_path = os.path.join(dataset_dir, filename)\n",
    "    with tf.gfile.Open(file_path, 'w') as f:\n",
    "        f.write('{}, {}, {}'.format(shape[0], shape[1], shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_shape_file(MODEL, dataset_dir):\n",
    "    '''Read the shape file of MODEL.\n",
    "    \n",
    "    Args:\n",
    "        MODEL(str): model name\n",
    "        dataset_dir(str): file directory\n",
    "        \n",
    "    Return:\n",
    "        shape(list): [height, width, channel]\n",
    "    '''\n",
    "    filename = 'shape_' + MODEL + '.txt'\n",
    "    file_path = os.path.join(dataset_dir, filename)\n",
    "    with tf.gfile.Open(file_path, 'rb') as f:\n",
    "        lines = f.read().decode()\n",
    "        return list(map(int, lines.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def array_to_tfexample(array_b, class_id, img_b):\n",
    "    return tf.train.Example(features=tf.train.Features(feature={\n",
    "        'array/encoded': bytes_feature(array_b),\n",
    "        'array/filename': bytes_feature(img_b),\n",
    "        'array/class/label': int64_feature(class_id)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def array_to_tfrecord(array, class_id, img, writer):\n",
    "    array = array.astype(np.float32)\n",
    "    array_b = array.tostring()\n",
    "    img_b = img.encode()\n",
    "    example = array_to_tfexample(array_b, class_id, img_b)\n",
    "    writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_name = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading resnet_v2_50_2017_04_14.tar.gz 100.0%\n",
      "Successfully downloaded resnet_v2_50_2017_04_14.tar.gz 286441851 bytes.\n",
      "Checkpoint for resnet_v2_50 is ready!\n",
      "File name: ../checkpoints/resnet_v2_50.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = ckpt_maker(MODEL, checkpoints_dir=checkpoints_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int64_feature(values):\n",
    "  \"\"\"Returns a TF-Feature of int64s.\n",
    "\n",
    "  Args:\n",
    "    values: A scalar or list of values.\n",
    "\n",
    "  Returns:\n",
    "    a TF-Feature.\n",
    "  \"\"\"\n",
    "  if not isinstance(values, (tuple, list)):\n",
    "    values = [values]\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n",
    "\n",
    "def float_feature(values):\n",
    "    if not isinstance(values, (tuple, list)):\n",
    "        values = [values]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n",
    "\n",
    "\n",
    "def bytes_feature(values):\n",
    "  \"\"\"Returns a TF-Feature of bytes.\n",
    "\n",
    "  Args:\n",
    "    values: A string.\n",
    "\n",
    "  Returns:\n",
    "    a TF-Feature.\n",
    "  \"\"\"\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from ../checkpoints/resnet_v2_50.ckpt\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/17940 [00:00<2:06:38,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    # load model\n",
    "    model = nets_factory.get_network_fn(MODEL, num_classes=num_classes, is_training=False)\n",
    "    # get image size\n",
    "    image_size = model.default_image_size\n",
    "    \n",
    "\n",
    "    dataset = get_split(split_name=split_name, \n",
    "                        dataset_dir=dataset_dir, \n",
    "                        file_pattern=file_pattern, \n",
    "                        file_pattern_for_counting='drivers', \n",
    "                        items_to_descriptions=None)\n",
    "    images, _, labels, image_names = load_batch(\n",
    "        dataset=dataset, \n",
    "        batch_size=1, \n",
    "        MODEL=MODEL, \n",
    "        height=image_size, \n",
    "        width=image_size, \n",
    "        is_training=False)\n",
    "\n",
    "    _, endpoints = model(images)\n",
    "    \n",
    "    base_scope = base_layer(MODEL)\n",
    "\n",
    "    net = endpoints[base_scope]\n",
    "    shape = net.get_shape()[1:]\n",
    "    write_shape_file(shape, MODEL, tf_dir)\n",
    "    \n",
    "    exclusion = exclude_scope(MODEL)\n",
    "    variables_to_restore = slim.get_variables_to_restore(exclude=exclusion)\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    def restore_fn(sess):\n",
    "        return saver.restore(sess, checkpoint_file)\n",
    "\n",
    "    sv = tf.train.Supervisor(logdir=log_dir, summary_op=None, saver=None, init_fn=restore_fn)\n",
    "\n",
    "    with sv.managed_session() as sess:\n",
    "        filename = tf_dir + '/' + split_name + '.tfrecord'\n",
    "        with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "            for i in tqdm(range(dataset.num_samples)):\n",
    "                arrays, array_labels, img_names = sess.run([net, labels, image_names])\n",
    "                array, array_label, img_name = np.array(arrays[0]), array_labels[0], img_names[0].decode()\n",
    "                array_to_tfrecord(array, array_label, img_name, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_tv(MODEL, split_name, dataset_dir, file_pattern, tf_dir, checkpoint_file, log_dir):\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "        # load model\n",
    "        model = nets_factory.get_network_fn(MODEL, num_classes=10, is_training=False)\n",
    "        # get image size\n",
    "        image_size = model.default_image_size\n",
    "\n",
    "\n",
    "        dataset = get_split(split_name=split_name, \n",
    "                            dataset_dir=dataset_dir, \n",
    "                            file_pattern=file_pattern, \n",
    "                            file_pattern_for_counting='drivers', \n",
    "                            items_to_descriptions=None)\n",
    "        images, _, labels, image_names = load_batch(\n",
    "            dataset=dataset, \n",
    "            batch_size=1, \n",
    "            MODEL=MODEL, \n",
    "            height=image_size, \n",
    "            width=image_size, \n",
    "            is_training=False)\n",
    "\n",
    "        _, endpoints = model(images)\n",
    "\n",
    "        base_scope = base_layer(MODEL)\n",
    "\n",
    "        net = endpoints[base_scope]\n",
    "        shape = net.get_shape()[1:]\n",
    "        write_shape_file(shape, MODEL, tf_dir)\n",
    "\n",
    "        exclusion = exclude_scope(MODEL)\n",
    "        variables_to_restore = slim.get_variables_to_restore(exclude=exclusion)\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        def restore_fn(sess):\n",
    "            return saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        sv = tf.train.Supervisor(logdir=log_dir, summary_op=None, saver=None, init_fn=restore_fn)\n",
    "\n",
    "        with sv.managed_session() as sess:\n",
    "            filename = tf_dir + '/' + split_name + '.tfrecord'\n",
    "            with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "                for i in tqdm(range(dataset.num_samples)):\n",
    "                    arrays, array_labels, img_names = sess.run([net, labels, image_names])\n",
    "                    array, array_label, img_name = np.array(arrays[0]), array_labels[0], img_names[0].decode()\n",
    "                    array_to_tfrecord(array, array_label, img_name, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_split_tv(split_name, dataset_dir, file_pattern=None, items_to_descriptions=None):\n",
    "    '''\n",
    "    Obtains the split - training or validation or test - to create a Dataset class for feeding the examples into a queue later on. \n",
    "    This function will set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
    "    Your file_pattern is very important in locating the files later.\n",
    "    INPUTS:\n",
    "    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "    - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "    - file_pattern_for_counting(str): the string name to identify your tfrecord files for counting\n",
    "    OUTPUTS:\n",
    "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
    "    '''\n",
    "\n",
    "    # First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation', 'test']:\n",
    "        raise ValueError('split name {} was not recognized.'.format(split_name))\n",
    "\n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = os.path.join(dataset_dir, file_pattern.format(split_name))\n",
    "\n",
    "    #Count the total number of examples in all of these shard\n",
    "    num_samples = 0\n",
    "    file_pattern_for_counting = split_name\n",
    "    tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.startswith(file_pattern_for_counting)]\n",
    "    if len(tfrecords_to_count) == 0:\n",
    "        raise ValueError('There is no dataset.')\n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
    "            num_samples += 1\n",
    "\n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "        'array/encoded': tf.FixedLenFeature([], tf.string),\n",
    "        'array/filename': tf.FixedLenFeature([], tf.string),\n",
    "        'array/class/label': tf.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    items_to_handlers = {\n",
    "        'array': slim.tfexample_decoder.Tensor('array/encoded'),\n",
    "        'filename': slim.tfexample_decoder.Tensor('array/filename'),\n",
    "        'label': slim.tfexample_decoder.Tensor('array/class/label')\n",
    "        }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "    \n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = file_pattern_path,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        num_samples = num_samples,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batch_tv(dataset, batch_size, MODEL, shape, is_training=True):\n",
    "    '''\n",
    "    Loads a batch for training.\n",
    "    INPUTS:\n",
    "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
    "    - batch_size(int): determines how big of a batch to train\n",
    "    - height(int): the height of the image to resize to during preprocessing\n",
    "    - width(int): the width of the image to resize to during preprocessing\n",
    "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
    "    OUTPUTS:\n",
    "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
    "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
    "    '''\n",
    "    #First create the data_provider object\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset,\n",
    "        # common_queue_capacity = 24 + 3 * batch_size,\n",
    "        common_queue_capacity = 2 * batch_size,\n",
    "        common_queue_min = 1,\n",
    "        shuffle=is_training)\n",
    "\n",
    "    #Obtain the raw image using the get method\n",
    "    array, filename, label = data_provider.get(['array', 'filename', 'label'])\n",
    "\n",
    "    array = tf.decode_raw(array, tf.float32)\n",
    "    array = tf.reshape(array, shape)\n",
    "\n",
    "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
    "    arrays, labels, filenames = tf.train.batch(\n",
    "        [array, label, filename],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=1,\n",
    "        capacity=2 * batch_size,\n",
    "        allow_smaller_final_batch = True)\n",
    "\n",
    "    return arrays, labels, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL = 'resnet_v2_50'\n",
    "\n",
    "tf_dir = '../transfer-value-tf/' + MODEL\n",
    "if not tf.gfile.Exists(tf_dir):\n",
    "    tf.gfile.MakeDirs(tf_dir)\n",
    "    \n",
    "dataset_dir = '../dataset_tf'\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "file_pattern = 'drivers_{}_*.tfrecord'\n",
    "\n",
    "checkpoints_dir = '../checkpoints'\n",
    "\n",
    "log_dir = '../log/transfer-value/' + MODEL\n",
    "if not tf.gfile.Exists(log_dir):\n",
    "    tf.gfile.MakeDirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_tmp_dir = '../log/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 7, 2048]\n",
      "Tensor(\"batch:0\", shape=(?, 7, 7, 2048), dtype=float32)\n",
      "Tensor(\"batch:1\", shape=(?,), dtype=int64)\n",
      "Tensor(\"batch:2\", shape=(?,), dtype=string)\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Recording summary at step None.\n",
      "(1, 7, 7, 2048)\n",
      "(7, 7, 2048)\n",
      "9\n",
      "b'img_30706.jpg'\n",
      "(1, 7, 7, 2048)\n",
      "(7, 7, 2048)\n",
      "8\n",
      "b'img_49247.jpg'\n",
      "(1, 7, 7, 2048)\n",
      "(7, 7, 2048)\n",
      "5\n",
      "b'img_71369.jpg'\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    dataset = get_split_tv(split_name=split_name, \n",
    "                        dataset_dir=tf_dir, \n",
    "                        file_pattern='train*.tfrecord', \n",
    "                        items_to_descriptions=None)\n",
    "    \n",
    "    shape = read_shape_file(MODEL, tf_dir)\n",
    "    print(shape)\n",
    "    \n",
    "    arrays, labels, filenames = load_batch_tv(\n",
    "        dataset=dataset, \n",
    "        batch_size=1, \n",
    "        MODEL=MODEL, \n",
    "        shape=shape, \n",
    "        is_training=False)\n",
    "    print(arrays)\n",
    "    print(labels)\n",
    "    print(filenames)\n",
    "\n",
    "    sv = tf.train.Supervisor(logdir=log_tmp_dir)\n",
    "\n",
    "    with sv.managed_session() as sess:\n",
    "        for i in range(dataset.num_samples):\n",
    "            arrs, lbs, fns = sess.run([arrays, labels, filenames]) \n",
    "            print(arrs.shape)\n",
    "            arr, lb, fn = arrs[0], lbs[0], fns[0]\n",
    "            print(arr.shape)\n",
    "            print(lb)\n",
    "            print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
