{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from preprocessing import inception_preprocessing\n",
    "from nets.inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "from nets import inception\n",
    "import os\n",
    "import time\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "from datasets import dataset_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#================ DATASET INFORMATION ======================\n",
    "#State dataset directory where the tfrecord files are located\n",
    "dataset_dir = 'drivers_data'\n",
    "\n",
    "#State where your log file is at. If it doesn't exist, create it.\n",
    "log_dir = 'log'\n",
    "\n",
    "if not tf.gfile.Exists(log_dir):\n",
    "    tf.gfile.MakeDirs(log_dir)\n",
    "\n",
    "#State where your checkpoint file is\n",
    "url = \"http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz\"\n",
    "checkpoints_dir = 'checkpoints'\n",
    "checkpoint_file = 'checkpoints/inception_resnet_v2_2016_08_30.ckpt'\n",
    "\n",
    "if not tf.gfile.Exists(checkpoints_dir):\n",
    "    tf.gfile.MakeDirs(checkpoints_dir)\n",
    "\n",
    "if not tf.gfile.Exists(checkpoint_file):\n",
    "    dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)\n",
    "\n",
    "#State the image size you're resizing your images to. We will use the default inception size of 299.\n",
    "image_size = inception.inception_resnet_v2.default_image_size\n",
    "\n",
    "#State the number of classes to predict:\n",
    "num_classes = 10\n",
    "\n",
    "#State the labels file and read it\n",
    "labels_file = 'drivers_data/labels.txt'\n",
    "labels = open(labels_file, 'r')\n",
    "\n",
    "#Create a dictionary to refer each label to their string name\n",
    "labels_to_name = {}\n",
    "for line in labels:\n",
    "    label, string_name = line.split(':')\n",
    "    string_name = string_name[:-1] #Remove newline\n",
    "    labels_to_name[int(label)] = string_name\n",
    "\n",
    "# Create the file pattern of your TFRecord files so that it could be recognized later on\n",
    "# file_pattern = 'drivers_%s_*.tfrecord'\n",
    "file_pattern = 'drivers_{}_*.tfrecord'\n",
    "\n",
    "#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.\n",
    "items_to_descriptions = {\n",
    "    'image': 'A 3-channel RGB coloured driver image.',\n",
    "    'label': 'A label of status of driver -- c0, c1, c2, c3, c4, c5, c6, c7, c8, c9'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================= TRAINING INFORMATION ==================\n",
    "# State the number of epochs to train\n",
    "num_epochs = 10\n",
    "\n",
    "# State your batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Learning rate information and configuration (Up to you to experiment)\n",
    "initial_learning_rate = 0.001\n",
    "learning_rate_decay_factor = 0.7\n",
    "num_epochs_before_decay = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create a function that creates a Dataset class which will give us many TFRecord files to feed in the examples into a queue in parallel.\n",
    "def get_split(split_name, dataset_dir, file_pattern=file_pattern, file_pattern_for_counting='drivers'):\n",
    "    '''\n",
    "    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n",
    "    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
    "    Your file_pattern is very important in locating the files later. \n",
    "    INPUTS:\n",
    "    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "    - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "    - file_pattern_for_counting(str): the string name to identify your tfrecord files for counting\n",
    "    OUTPUTS:\n",
    "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
    "    '''\n",
    "\n",
    "    # First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation']:\n",
    "        raise ValueError('split name {} was not recognized.'.format(split_name))\n",
    "\n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = os.path.join(dataset_dir, file_pattern.format(split_name))\n",
    "\n",
    "    #Count the total number of examples in all of these shard\n",
    "    num_samples = 0\n",
    "    file_pattern_for_counting = file_pattern_for_counting + '_' + split_name\n",
    "    tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.startswith(file_pattern_for_counting)]\n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
    "            num_samples += 1\n",
    "\n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
    "      'image/class/label': tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    #Create the items_to_handlers dictionary for the decoder.\n",
    "    items_to_handlers = {\n",
    "    'image': slim.tfexample_decoder.Image(),\n",
    "    'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "    #Create the labels_to_name file\n",
    "    labels_to_name_dict = labels_to_name\n",
    "\n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = file_pattern_path,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        # num_readers = 4,\n",
    "        num_samples = num_samples,\n",
    "        num_classes = num_classes,\n",
    "        labels_to_name = labels_to_name_dict,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(dataset, batch_size, height=image_size, width=image_size, is_training=True):\n",
    "    '''\n",
    "    Loads a batch for training.\n",
    "    INPUTS:\n",
    "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
    "    - batch_size(int): determines how big of a batch to train\n",
    "    - height(int): the height of the image to resize to during preprocessing\n",
    "    - width(int): the width of the image to resize to during preprocessing\n",
    "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
    "    OUTPUTS:\n",
    "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
    "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
    "    '''\n",
    "    #First create the data_provider object\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset,\n",
    "        # common_queue_capacity = 24 + 3 * batch_size,\n",
    "        common_queue_capacity = 2 * batch_size,\n",
    "        common_queue_min = 24)\n",
    "\n",
    "    #Obtain the raw image using the get method\n",
    "    raw_image, label = data_provider.get(['image', 'label'])\n",
    "\n",
    "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "    image = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)\n",
    "\n",
    "    #As for the raw images, we just do a simple reshape to batch it up\n",
    "    raw_image = tf.expand_dims(raw_image, 0)\n",
    "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
    "    raw_image = tf.squeeze(raw_image)\n",
    "\n",
    "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
    "    images, raw_images, labels = tf.train.batch(\n",
    "        [image, raw_image, label],\n",
    "        batch_size=batch_size,\n",
    "        # num_threads = 4,\n",
    "        num_threads=1\n",
    "        # capacity = 4 * batch_size,\n",
    "        capacity=2 * batch_size\n",
    "        allow_smaller_final_batch = True)\n",
    "\n",
    "    return images, raw_images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build graph and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    # creat dataset and load batches\n",
    "    dataset = get_split('train', dataset_dir, file_pattern=file_pattern)\n",
    "    imges, _, labels = load_batch(dataset, batch_size=batch_size)\n",
    "    \n",
    "    num_batches_per_epoch = int(dataset.num_samples / batch_size)\n",
    "    num_steps_per_epoch = num_batches_per_epoch\n",
    "    decay_steps = int(num_epochs_before_decay * num_steps_per_epoch)\n",
    "    \n",
    "    # creat the model\n",
    "    with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "        logits, end_points = inception_resnet_v2(images, num_classes=dataset.num_classes, is_training=True)\n",
    "    \n",
    "    # define scopes to excluded\n",
    "    exclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']\n",
    "    variables_to_restore = slim.get_variables_to_restore(exclude=exclude)\n",
    "    \n",
    "    # one-hot-encodeing of the labels\n",
    "    one_hot_labels = slim.one_hot_encoding(labels, dadtaset.num_classes)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits=logits)\n",
    "    total_loss = tf.losses.get_total_loss()\n",
    "    \n",
    "    # create global step for mornitoring\n",
    "    global_step = get_or_create_global_step()\n",
    "    \n",
    "    # define decaying learning rate\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=initial_learning_rate, \n",
    "                                               global_step=global_step, \n",
    "                                               decay_steps=decay_steps, \n",
    "                                               decay_rate=learning_rate_decay_factor, \n",
    "                                               staircase=True)\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # create training operator\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    \n",
    "    # the predictions\n",
    "    probabilities = end_points['Predictions']\n",
    "    predictions = tf.argmax(end_points['Predictions'], 1)\n",
    "    accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "    metrics_op = tf.group(accuracy_update, probabilities)\n",
    "    \n",
    "    # summaries\n",
    "    tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    my_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # define training step function\n",
    "    def train_step(sess, train_op, global_step):\n",
    "        start_time = time.time()\n",
    "        total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n",
    "        \n",
    "        return total_loss, global_step_count\n",
    "    \n",
    "    # create saver function to restore variables from a checkpoint file\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    def restore_fn(sess):\n",
    "        return saver.restore(sess, checkpoint_file)\n",
    "    \n",
    "    sv = tf.train.Supervisor(logdir=log_dir, summary_op=None, init_fn=restore_fn)\n",
    "    \n",
    "    with sv.managed_session() as sess:\n",
    "        for step in xrange(num_steps_per_epoch * num_epochs):\n",
    "            if step % num_batches_per_epoch == 0:\n",
    "                logging.info('Epoch {}/{}'.format(step/num_batches_per_epoch + 1, num_epochs))\n",
    "                learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n",
    "                logging.info('Current Learning Rate: {}'.format(learning_rate_value))\n",
    "                logging.info('Current Streaming Accuracy: {}'.format(accuracy_value))\n",
    "                \n",
    "            if step % 10 == 0:\n",
    "                loss, step = train_step(sess, train_op, sv.global_step)\n",
    "                summaries = sess.run(my_summary_op)\n",
    "                sv.summary_computed(sess, summaries)\n",
    "                logging.info('global step {}: loss: {}'.format(step, loss))\n",
    "            else:\n",
    "                loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "                \n",
    "        logging.info('Final Loss: {}'.format(loss))\n",
    "        logging.info('Final Accuracy: {}'.format(sess.run(accuracy)))\n",
    "        \n",
    "        logging.info('Training finished! Saving model to disk.')\n",
    "        sv.saver.save(sess, sv,save_path, global_step=sv.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
